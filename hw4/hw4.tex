\documentclass[12pt]{article}

\usepackage[pdftex]{graphicx}
\usepackage{cancel}
\usepackage[margin=4cm]{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\course}{COMS 474}

\setcounter{secnumdepth}{0} % Disable section/subsection numbering
\hyphenpenalty 10000 % Prevent words from being broken over multiple lines
\exhyphenpenalty 10000 % Prevent words from being broken over multiple lines

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\title{ \course \\\large Homework 4 }
\author{ Haadi Majeed }
\date{Spring 2022}


\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\lhead{\course}
\chead{Haadi Majeed}
\rhead{Page \thepage}

\begin{document}
\maketitle
\pagebreak

% Optional TOC
%\tableofcontents
\pagebreak
\section{Problem 1}
 [24 points total (5,5,3,3,4,4)]\\
Suppose you use lasso fit to a linear model for a data set. Let $\beta^*(\lambda)$ denote the lasso solution for a specific $\lambda$ (i.e. the coeffcient vector you get for that $\lambda$).
\\\\Provide explanations for your answers to the following questions.

\subsection{A}
Describe how the \underline{training} MSE changes as a fucntion of $\lambda$, including $\lambda = 0$ and as $\lambda \rightarrow \infty$\\\\

\subsection{B}
Describe how the \underline{hold-out} MSE changes as a fucntion of $\lambda$, including $\lambda = 0$ and as $\lambda \rightarrow \infty$\\\\


\subsection{C}
Describe $\beta^*(0)$.\\\\

\subsection{D}
Describe what happens to $\beta^*(0)$ as $\lambda$ grows.\\\\

\subsection{E}
If you used ridge regression instead of lasso, explain how your answers to (a).-(d). would differ. \\\\

\subsection{F}
We discussed the “constrained form” of lasso, with a constraint of the form
\begin{center}
    \[\sum_{i=1}^{p}|\beta_j| \leq t\]
\end{center}
Which value, or limiting value, of $t$ coresponds to $\lambda = 0$ and which corresponds to $\lambda \rightarrow \infty$\\\\

\pagebreak
\section{Problem 2}
 [15 points]\\
You have already seen formulas for the best intercept in linear models when there are no
features $p = 0$ and a single feature $p = 1$. You will now look at what happens with $p$ features
when we center the data.\\
Recall that “centering” a feature means subtracting its mean. For example, if the sample values
for feature $X_4$ are $\begin{bmatrix} 5 \\0\\1\end{bmatrix}$, which has a mean of $2$, \\we could replace it with $\begin{bmatrix} 5 - 2\\0 - 2\\1 - 2\end{bmatrix} = \begin{bmatrix} 3\\-2\\-1\end{bmatrix}$ which has a mean of $0$. Thus if feature $X_4$ is centred, then $\sum^n_{i=1}X_4(i) = 0$.\\\\

What is the value of the intercept $\beta^*_0$ in the ordinary least squares solution, i.e.
\begin{center}
    \[
        (\beta^*_0, \beta^*_1, \dots, \beta^*_p) = \underset{(\beta^*_0, \beta^*_1, \dots, \beta^*_p)\epsilon\mathbb{R}^{p+1}}{\text{arg min}}\frac{1}{n}\sum_{i = 1}^{n} \bigg( Y(i) - \beta_0 \sum_{j=1}^p\beta_jX_j(i)\bigg) ^2
    \]
\end{center}
when the features ${X_1, \dots, X_p}$ are all centered? (e.g. $\sum_{i=1}^nX_j(i) = 0$ for $ j = 1, \dots, p$). (You do not need to use a second derivative test or solve for {$\beta_1^*, \dots, \beta_p^*$}, just use the first derivative test $0 = \frac{\partial}{\partial\beta_0}$ MSE).

%--/Paper--

\end{document}